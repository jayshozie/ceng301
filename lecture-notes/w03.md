MIT License
Copyright (c) 2025 Emir Baha Yıldırım
Please see the LICENSE file for more details.

> [!NOTE]
> This is the point where the course actually starts. I will have to use LaTeX
> and graphs to actually create good and legible lecture notes, so it may or may
> not take more than the usual rate.

-------------------------------------------------------------------------------

# Algorithm Analysis

## Algorithm

- An *algorithm* is a set of definite instructions to be followed to solve a
problem.
    - There can be more than one solution (algorithm) to solve a given problem.
    - An algorithm can be implemented using different programming languages on
    different platforms.

- An algorithm must be *correct*, meaning that it should solve the problem.
    - For example, a sorting algorithm should solve the problem correctly even
    if the the input is already sorted, or it contains repeated elements. It
    should correctly output the sorted version of the input without raising
    errors.

## Algorithmic Performance

There are **2** aspects of algorithmic performance.
1. **Time:** Instructions take time.
    - How fast does the algorithm perform?
    - What affects its runtime?
2. **Space:** Data structures take space.
    - What kind of data structures can be used?
    - How does choice of data structure affect the runtime?

> [!NOTE]
> **Runtime:** the amount of time the algorithm takes to complete its execution

We will mostly focus on time. Thus learning how to estimate the time required
for an algorithm and how to reduce that time.

## Analysis of Algorithms

When we analyze algorithms, we employ mathematical techniques that analyze
algorithms independent of *specific implementations*, *computers*, or *data*.

To analyze algorithms, we first start to count the number of significant
operations in a particular solution to assess its efficiency, then, we will
express the efficiency of algorithms using growth functions.

## The Running Time of Algorithms

Each instruction (operation) in an algorithm (program) has a cost, because each
operation takes a certain time, whether it's being run on an old
[Intel 8086](https://en.wikipedia.org/wiki/Intel_8086), or a top-of-the-line
[AMD Ryzen 9950X3D](https://en.wikipedia.org/wiki/AMD_Ryzen_9950X3D). Although,
it will be vastly different lengths of time, it will grow just the same. We'll
see what I mean in a few sections.
```cpp
count = count + 1; // takes a certain amount of time on both cpus, but is constant
```
A sequence of operations:
```cpp
count = count + 1; // Cost: c1
sum = sum + count; // Cost: c2
```
Total cost of the above program would be $`c_1 + c_2`$, independent of what
$`c_1`$ or $`c_2`$ actually is.

## Runtime Analysis

Runtime analysis is the process of determining the amount of time an algorithm
takes to complete as a function of the length of the input.
```cpp
// Example: Simple If-Statement
                   // Cost    Times
if (n < 0) {       //  c1       1
    absval = -n;   //  c2       1
}
else {
    absval = n;    //  c3       1
}
```
$`c_T \le c_1 + max(c_2, c_3)`$

```cpp
// Example: Simple Loop
                      // Cost    Times
int i = 1;            //  c1       1
int sum = 0;          //  c2       1
while (i <= n) {      //  c3      n+1
    i = i + 1;        //  c4       n
    sum = sum + 1;    //  c5       n
}
```
$`c_T = c_1 + c_2 + (n+1).c_3 + n.c_4 + n.c_5`$

Thus, the time required for this algorithm is proportional to $`n`$. So, even
if the 8086 takes `10` times longer to execute each instruction than the Ryzen,
both will have their runtimes grow linearly with $`n`$.

```cpp
// Example: Nested Loop
                          // Cost    Times
int i = 1;                //  c1       1
int sum = 0;              //  c2       1
while (i <= n) {          //  c3      n+1
    int j = 1;            //  c4       n
    while (j <= n) {      //  c5    n*(n+1)
        sum = sum + i;    //  c6      n*n
        j = j + 1;        //  c7      n*n
    }
    i = i + 1;            //  c8       n
}
```
$`c_T = c_1 + c_2 + (n+1).c_3 + n.c_4 + n.(n+1).c_5 + n.n.c_6 + n.n.c_7 + n.c_8`$
$`c_T = c_1 + c_2 + (n+1).c_3 + n.c_4 + n.(n+1).c_5 + {n^2}.c_6 + {n^2}.c_7 + n.c_8`$

Thus, the time required for this algorithm is proportional to $`n^2`$.

## Example: Nested Loop

**Problem:** Given $`n`$ numbers in an array `A`, calculate the sum of all
distinct pairwise multiplications.
```cpp
// A is an array of integers of size n
double sum = 0.0;
for (int i = 0; i < n; i++) {
    for (int j = i; j < n; j++) {
        sum += A[i] * A[j];
    }
}
```
$`\sum_{i=0}^{n-1} (n - i) = n + (n-1) + (n-2) + ... + 1 = \frac{n(n+1)}{2}`$

Thus, the time required for this algorithm is proportional to $`n^2`$.

## Exercise

Which of these loops take constant time regardless of the value of $`n`$?
1) `for (i=n/10; i<n; i++) sum+=i;`
2) `for (i=0; i<n; i+=n/10) sum++;`
3) `for (i=n; i<2*n; i++) sum--;`
4) `for (i=0; i<n; i+=10) sum++;`
5) `for (i=0; i<n/10; i+=10) sum*=2;`

<details>
    <summary> Solution </summary>

Answer: **2.**

*1.* Would grow linearly with $`n`$, because $`i`$ starts at $`\frac{n}{10}`$
but grows by $`1`$ at every iteration. If $`n`$ were $`10`$ times bigger, the
algorithm would take $`10`$ times longer.

**2.** Would always take up to $`10`$ steps, because $`i`$ starts at $`0`$, but
grows by $`\frac{n}{10}`$ at every iteration, so even if $`n`$ was $`10`$ times
bigger, the algorithm would still take $`10`$ steps.

*3.* Again, would grow linearly, because $`n`$ could be $`10`$ times bigger and
$`i`$ still grows by $`1`$ at every iteration.

*4.* This option would grow linearly, too, because although we're now
incrementing $`i`$ by $`10`$ at every step, if $`n`$ was $`10`$ times bigger,
it would take us $`10`$ times more time.

*5.* Same thing as option 4.
</details>

## Algorithm Growth Rates

- We measure an algorithm's time requirement as a function of the *problem size*.
    - Problem size depends on the application. (E.g. number of elements in a
    list for a sorting algorithm, number of disks for towers of Hanoi, etc.)
- The most important thing to learn is how quickly the algorithm's time
requirement grows as a function of the problem size.
    - Algorithm `A` requires time, proportional to $`n^2`$.
    - Algorithm `B` requires time, proportional to $`n`$.
- An algorithm's proportional time requirement is known as ***growth rate***.
- We can compare the efficiency of two algorithms by comparing their growth
rates.

### Time Requirements as a Function of the Problem Size $`n`$
<a href="../slides/w03.pdf">
    <img
        src="./images/algorithm-growth-rates-1.png"
        alt="A graph of problem size and time required by algorithm, showing a linear and an exponential functions. Linear: Algorithm B requires 5 times n seconds, Exponential: Algorithm A requires n squared over 5 seconds. Graphs meet at n equals 25."
        style="width:35%;
        height:auto;">
</a>

### Running Times for Small Inputs of Different Functions
<a href="../slides/w03.pdf">
    <img
        src="./images/algorithm-growth-rates-2.png"
        alt="A graph of input size x equals n and running time, showing functions y=x, y=log(x), y=xlog(x), y=x^2, y=x^3, y=2^x."
        style="width:35%;
        height:auto;">
</a>

### Running Times for Large Inputs of Different Functions
<a href="../slides/w03.pdf">
    <img
        src="./images/algorithm-growth-rates-3.png"
        alt="A graph of input size x equals n and running time, showing functions y=x, y=log(x), y=xlog(x), y=x^2, y=x^3, y=2^x."
        style="width:35%;
        height:auto;">
</a>

## Big-O Notation

The Big-O notation is a mathematical notation that describes the *limiting*
behavior of a function when the argument tends towards a particular value or
infinity. We use Big-O notation to describe the computation time
(**complexity**) of algorithms using algebraic terms. `O` stands for `order`,
as in `order of magnitude`.

### Formal Definition

$`O(g(n)) = \{f(n) \text{: there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \le f(n) \le c.g(n) \text{ for all } n \ge n_0\}`$
<a href="../slides/w03.pdf">
    <img
        src="./images/big-o-notation.png"
        alt="A graph showing functions f(n) and cg(n)."
        style="width:35%;
        height:auto;">
</a>

$`g(n) \text{ is an \textit{asymptotic upper bound} for } f(n). \text{If } f(n) \in O(g(n)) \text{, we write } f(n) = O(g(n))`$

### Big-O Example

If an algorithm requires $`2n^2 - 3n + 10`$ seconds to solve a problem size 
$`n`$ and constants $`c \text{ and } n_0`$ exist such that.
```math
\begin{equation}
2n^2 - 3n + 10 \le cn^2 \text{ for all } n \ge n_0
\end{equation}
```
In fact, for $`c = 3`$ and $`n_0 = 3`$:
```math
\begin{equation}
2n^2 - 3n + 10 \le 3n^2 \text{ for all } n \ge 3
\end{equation}
```
Thus, we say that the algorithm requires no more than $`3n^2`$ steps for
$`n \ge 3`$, so it is $`O(n^2)`$.
- The fastest growing term is $`2n^2`$.
- The constant $`2`$ can be ignored.

### Order of Terms

If we graph $`0.0001n^2`$ against $`10000n`$, the linear term would be larger
for a long time, but the quadratic one would eventually catch up (here at
$`n = 10^8`$).

From calculus we know that:
```math
\begin{equation}
\lim_{n\to\infty} \frac{10000n}{0.00001n^2} = \lim_{n\to\infty} \frac{10^8}{n} = 0
\end{equation}
```

As you can see, any quadratic (with a positive leading coefficient) will
eventually beat any linear. So the linear term in a quadratic function
eventually doesn't matter.

Consider the function $`n^4 + 100n^2 + 500 = O(n^4)`$
| $`n`$  | $`n^4`$           | $`100n^2`$   | $`500`$ | $`f(n)`$          |
|:-------|:------------------|:-------------|:--------|:------------------|
| 1      | 1                 | 100          | 500     | 601               |
| 10     | 10,000            | 10,000       | 500     | 20,500            |
| 100    | 100,000,000       | 1,000,000    | 500     | 101,000,500       |
| 1,000  | 1,000,000,000,000 | 100,000,000  | 500     | 1,000,100,000,500 |

The growth of a polynomial in $`n`$ as $`n`$ increases, depends primarily on
the degree (the highest order term), and not on the leading constant or the
low-order terms.

### Big-O Summary
- Write down the cost function, i.e., number of instructions in terms of the
problem size $`n`$.
    - Specifically, focus on the loops and find out how many iterations the
    loops run.
- Find the highest order term.
- Ignore the constant scaling factor.
- Now, you have a Big-O notation.

### Common Growth Rates
| Function     | Growth Rate Name          |
|:-------------|--------------------------:|
| $`c`$        | Constant                  |
| $`log(n)`$   | Logarithmic               |
| $`log^2(n)`$ | Log-squared               |
| $`n`$        | Linear                    |
| $`n.log(n)`$ | Log-linear (Linearithmic) |
| $`n^2`$      | Quadratic                 |
| $`n^3`$      | Cubic                     |
| $`2^n`$      | Exponential               |

### Growth Rate Functions

If an algorithm takes $`1 \text{ second}`$ to run with the problem size $`8`$,
what is the time requirement (approximately) for that algorithm with the
problem size $`16`$?
If its order is:

$`O(1) \rightarrow T(n) = 1\,\text{second}`$

$`O(\log_2{n}) \rightarrow T(n) = \frac{1\times\log_2(16)}{\log_2{8}} = \frac{4}{3}\,\text{seconds}`$

$`O(n) \rightarrow T(n) = \frac{1\times 16}{8} = 2\,\text{seconds}`$

$`O(n\times\log_2{n}) \rightarrow T(n) = \frac{1\times 16\times\log_2{16}}{8\times\log_2{8}} = \frac{8}{3}\,\text{seconds}`$

$`O(n^2) \rightarrow T(n) = \frac{1\times 16^2}{8^2} = 4\,\text{seconds}`$

$`O(n^3) \rightarrow T(n) = \frac{1\times 16^3}{8^3} = 8\,\text{seconds}`$

$`O(2^n) \rightarrow T(n) = \frac{1\times 2^{16}}{2^8} = 2^8\,\text{seconds}\ (= 256\,\text{seconds})`$
<!-- I couldn't find a better solution for this. If you know a better solution, open up a PR, please. -->

### Logarithmic Cost $`O(\log{n})`$

```cpp
for(int i = 1; i < n; i*=2) { ... }  // base 2
for(int i = 1; i < n; i<<=1) { ... } // base 2
for(int i = n; i > 0; i/=3) { ... }  // base 3
for(int i = n; i > 0; i>>=2) { ... } // base 4
```
The base doesn't matter, because:

$`O(\log_2{n}) = \frac{O(\ln{n})}{O(\ln{2})} = O(\ln{n})`$

$`\text{Change of Base} \rightarrow \text{Base } e \text{ natural log}`$

### Motivation for Other Asymptotic Bounds

```markdown
Algorithm **foo**
    **for** i=1 to n do
        **for** j=1 to n do
            do something...
        end for
    end for
```
Runtime is $`O(n^3)`$
```markdown
Algorithm **bar**
    **for** i=1 to n do
        **for** j=1 to n do
            **for** k=1 to n do
                do something else...
            end for
        end for
    end for
```
Runtime is $`O(n^3)`$
- Conclusion: **foo** and **bar** have the same asymptotic runtime. What is
wrong? could algorithm **bar** be better? $`O(n^2) \text{ or } O(n)`$?

## Big-Omega ($`\Omega`$) Notation
$`\Omega(g(n)) = \{f(n) \text{: there exist positive constants } c \text{ and } n_0 \text{ such that } 0 \le cg(n) \le f(n) \text{ for all } n \ge n_0\}`$
<a href="../slides/w03.pdf">
    <img
        src="./images/big-omega-notation.png"
        alt="A graph showing functions f(n) and cg(n)."
        style="width:35%;
        height:auto;">
</a>

$`g(n) \text{ is an \textit{asymptotic lower bound} for } f(n).`$

- **Example:** $`\sqrt{n} = \Omega(\log{n}) \text{, with } c = 1 \text{ and } n_0 = 16.`$

## Big-Theta ($`\Theta`$) Notation
$`\Theta(g(n)) = \{f(n) \text{: there exist positive constants } c_1, c_2 \text{, and } n_0
\text{ such that } 0 \le c_1\times g(n) \le f(n) \le c_2\times g(n) \text{ for all } n \ge n_0\}`$
<a href="../slides/w03.pdf">
    <img
        src="./images/big-theta-notation.png"
        alt="A graph showing functions f(n), c1 times g(n), and c2 times g(n)."
        style="width:35%;
        height:auto;">
</a>

$`g(n) \text{ is an \textit{asymptotically tight bound} for } f(n).`$

- **Example:** $`\frac{n^2}{2}-2n = \Theta(n^2), \text{ with } c_1 = \frac{1}{4}, c_2 = \frac{1}{2} \text{, and } n_0 = 8.`$

-------------------------------------------------------------------------------

# What to Analyze?

An algorithm can require different times to solve different problems of the
same size. For example, searching an item in an array of $`n`$ elements using
sequential search would grow linearly:
$`\text{Cost: } \rightarrow 1, 2, 3, ..., n`$

- *Worst-Case Analysis:* The <ins>**maximum**</ins> amount of time that an
algorithm requires to solve a problem size of $`n`$.
    - This gives an upper bound for the time complexity of an algorithm.
    - Normally, we try to find worst-case behavior of an algorithm.

- *Best-Case Analysis:* The <ins>**minimum**</ins> amount of time that an
algorithm requires to solve a problem of size $`n`$.
    - The best case behavior of an algorithm is *not* so useful.

- *Average-Case Analysis:* The <ins>**average**</ins> amount of time that an
algorithm requires to solve a problem of size $`n`$.
    - Sometimes, it can be difficult to find the average-case behavior of an
    algorithm.
    - We have to look at all possible data organizations of a given size $`n`$,
    and their distribution probabilities of these organizations.
    - ***Worst-case analysis is more common than average-case analysis.***

-------------------------------------------------------------------------------

# Sequential Search

```cpp
int sequentialSearch(const int a[], int item, int n) {
    for (int i= 0; i < n && a[i] != item; i++);

    if (i == n) {
        return -1;
    }
    return i;
}
```
- *Unsuccessful Search:* $`O(n)`$
- *Successful Search:*
    - **Best-Case:** *item* is in the first location of the array $`\rightarrow O(1)`$

    - **Worst-Case:** *item* is in the last location of the array $`\rightarrow O(n)`$

    - **Average-Case:** The number of key comparisons $`1, 2, 3, ... , n \rightarrow \frac{1}{n}\sum_{i=1}^{n} i = \frac{1}{n}\frac{n^2 + n}{2} \rightarrow O(n)`$

-------------------------------------------------------------------------------

# Binary Search

```cpp
int binarySearch(int a[], int size, int x) {
    int lo = 0;
    int hi = size - 1;
    int mi; // mid will be the index of target when found

    while (lo <= hi) {
        mi = (lo + hi)/2;
        if (a[mi] < x) {
            lo = mi + 1;
        }
        else if (a[mi] > x) {
            hi = mi - 1;
        }
        else {
            return mi;
        }
    }
    return -1;
}
```
- *Unsuccessful Search:* $`\text{Number of Iterations} = \lfloor{\log_2{n}} + 1 \rightarrow O(\log_2{n})`$
- *Successful Search:*
    - **Best-Case:** Number of iterations is $`1 \rightarrow O(1)`$

    - **Worst-Case:**  Number of iterations is $`\lfloor{\log_2{n}} + 1 \rightarrow O(\log_2{n})`$

    - **Average-Case:** $`\text{The average number of iterations} \le \log_2{n} \rightarrow O(\log_2{n}) `$

```
0  1  2  3  4  5  6  7  $`\leftarrow \text{ an array with size } 8`$

3  2  3  1  3  2  3  4  $`\leftarrow \text{ number of iterations}`$

$`\text{Average Number of Iterations} = \frac{21}{8} \le \log_2{8}`$
```
